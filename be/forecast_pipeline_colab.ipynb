{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd4bd72",
   "metadata": {},
   "source": [
    "# Forecasting pipeline (Colab-ready)\n",
    "# This notebook generates a synthetic ecommerce demand dataset and runs ARIMA and LSTM forecasting.\n",
    "# Author: Generated by script for Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0776f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install required packages (Colab-friendly). Note: reinstalling tensorflow may restart runtime.\n",
    "python -V\n",
    "pip -q install scikit-learn==1.3.2 tensorflow==2.13.0 statsmodels==0.14.6 pandas==2.3.3 matplotlib==3.10.8 numpy==2.3.5 nbformat -q\n",
    "python -c \"import sklearn, tensorflow as tf, statsmodels, pandas, matplotlib, numpy; print('sklearn', sklearn.__version__); print('tensorflow', tf.__version__); print('statsmodels', statsmodels.__version__); print('pandas', pandas.__version__); print('matplotlib', matplotlib.__version__); print('numpy', numpy.__version__)\"\n",
    "# Show if GPU available\n",
    "python -c \"import tensorflow as tf; print('GPU available:', tf.config.list_physical_devices('GPU'))\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic seeding for reproducibility\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "try:\n",
    "    tf.random.set_seed(SEED)\n",
    "    if hasattr(tf.config.experimental, 'enable_op_determinism'):\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "except Exception:\n",
    "    pass\n",
    "print('Seed set to', SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Imports used in the notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print('Imports OK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation (ported from generate_ecommerce_demand.py, compacted for notebook)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "START_DATE = '2022-01-01'\n",
    "END_DATE = '2024-12-31'\n",
    "SKUS = ['SKU_001','SKU_002','SKU_003']\n",
    "\n",
    "RANDOM_SEED=SEED\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "dates = pd.date_range(start=START_DATE,end=END_DATE,freq='D')\n",
    "n_days = len(dates)\n",
    "day_idx = np.arange(n_days)\n",
    "weekday = dates.weekday.values\n",
    "day_of_year = dates.dayofyear.values\n",
    "\n",
    "# Parameters\n",
    "BASE_DEMAND = {'SKU_001':80.0,'SKU_002':45.0,'SKU_003':18.0}\n",
    "BASE_PRICE = {'SKU_001':19.99,'SKU_002':9.99,'SKU_003':49.99}\n",
    "WEEKLY_STRENGTH = {'SKU_001':0.20,'SKU_002':0.28,'SKU_003':0.12}\n",
    "YEARLY_STRENGTH = {'SKU_001':0.30,'SKU_002':0.22,'SKU_003':0.08}\n",
    "TREND_FRACTION = {'SKU_001':0.12,'SKU_002':0.10,'SKU_003':0.18}\n",
    "\n",
    "rows = []\n",
    "for sku in SKUS:\n",
    "    base = BASE_DEMAND[sku]\n",
    "    base_price = BASE_PRICE[sku]\n",
    "    weekly_component = WEEKLY_STRENGTH[sku] * base * np.sin(2 * np.pi * day_idx / 7)\n",
    "    yearly_component = YEARLY_STRENGTH[sku] * base * np.sin(2 * np.pi * day_of_year / 365.25)\n",
    "    trend_total = base * TREND_FRACTION[sku]\n",
    "    trend = trend_total * (day_idx / float(n_days - 1))\n",
    "    noise_std = max(1.0, base * 0.12)\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_std, size=n_days)\n",
    "\n",
    "    promo_p = np.random.uniform(0.15,0.25)\n",
    "    if sku == 'SKU_003':\n",
    "        promotion_flag = np.zeros(n_days, dtype=int)\n",
    "        expected_promo_days = int(np.round(promo_p * n_days))\n",
    "        avg_len = 4\n",
    "        num_windows = max(1, expected_promo_days // max(1, avg_len))\n",
    "        starts = np.random.choice(np.arange(n_days), size=num_windows, replace=False)\n",
    "        for s in starts:\n",
    "            length = np.random.randint(2,8)\n",
    "            end = min(n_days, s + length)\n",
    "            promotion_flag[s:end] = 1\n",
    "        discount_pct = np.zeros(n_days)\n",
    "        window_days = promotion_flag.astype(bool)\n",
    "        if window_days.sum() > 0:\n",
    "            discount_pct[window_days] = np.random.uniform(0.08,0.25,size=window_days.sum())\n",
    "        promo_lift = np.random.uniform(0.25 * 1.2, 0.60 * 1.2)\n",
    "    else:\n",
    "        promotion_flag = np.random.binomial(1, promo_p, size=n_days)\n",
    "        promo_lift = np.random.uniform(0.25, 0.60)\n",
    "        will_discount = (np.random.rand(n_days) < 0.6) & (promotion_flag == 1)\n",
    "        discount_pct = np.zeros(n_days)\n",
    "        if will_discount.sum() > 0:\n",
    "            discount_pct[will_discount] = np.random.uniform(0.05,0.20,size=will_discount.sum())\n",
    "\n",
    "    # Price\n",
    "    price = np.empty(n_days, dtype=float)\n",
    "    price[0] = base_price * (1.0 + np.random.normal(0.0, 0.003))\n",
    "    if sku == 'SKU_003':\n",
    "        min_factor = 0.7\n",
    "        max_factor = 1.3\n",
    "        reversion_rate = 0.02\n",
    "        local_price_noise = 0.007 * 3.0\n",
    "    else:\n",
    "        min_factor = 0.8\n",
    "        max_factor = 1.2\n",
    "        reversion_rate = 0.04\n",
    "        local_price_noise = 0.007\n",
    "    for t in range(1, n_days):\n",
    "        drift = reversion_rate * (base_price - price[t - 1])\n",
    "        p_noise = price[t - 1] * np.random.normal(0.0, local_price_noise)\n",
    "        price[t] = price[t - 1] + drift + p_noise\n",
    "        if discount_pct[t] > 0:\n",
    "            price[t] = price[t] * (1.0 - discount_pct[t])\n",
    "        price[t] = float(np.clip(price[t], base_price * min_factor, base_price * max_factor))\n",
    "        price[t] += np.random.normal(0.0, 0.03)\n",
    "    price = np.round(price.clip(min=0.01), 2)\n",
    "\n",
    "    price_mean = price.mean()\n",
    "    PRICE_ELASTICITY_SKU = {'SKU_001':-1.5,'SKU_002':-1.5,'SKU_003':-0.8}\n",
    "    price_effect = PRICE_ELASTICITY_SKU[sku] * (price - price_mean) / price_mean * base\n",
    "\n",
    "    units_raw = base + trend + weekly_component + yearly_component + noise + price_effect\n",
    "\n",
    "    shocks = (np.random.rand(n_days) < 0.01)\n",
    "    if shocks.any():\n",
    "        shock_mults = np.random.uniform(1.5,3.0,size=shocks.sum())\n",
    "        units_raw[shocks] = units_raw[shocks] * shock_mults\n",
    "\n",
    "    units_promoted = units_raw * (1.0 + (promotion_flag * promo_lift))\n",
    "    lam = np.clip(units_promoted, a_min=0.1, a_max=None)\n",
    "    if sku == 'SKU_003':\n",
    "        shape = 0.9\n",
    "        lam_gamma = np.random.gamma(shape, (lam / shape).clip(min=1e-6))\n",
    "        units_final = np.random.poisson(lam_gamma).astype(int)\n",
    "    else:\n",
    "        units_final = np.random.poisson(lam).astype(int)\n",
    "    units_final = np.maximum(0, units_final)\n",
    "\n",
    "    rows.append(pd.DataFrame({\n",
    "        'date': dates.strftime('%Y-%m-%d'),\n",
    "        'sku_id': [sku] * n_days,\n",
    "        'units_sold': units_final,\n",
    "        'price': price,\n",
    "        'promotion_flag': promotion_flag\n",
    "    }))\n",
    "\n",
    "# Concatenate and save\n",
    "df_gen = pd.concat(rows, ignore_index=True).sort_values(['date','sku_id']).reset_index(drop=True)\n",
    "df_gen.to_csv('ecommerce_demand.csv', index=False)\n",
    "print('Wrote ecommerce_demand.csv with shape', df_gen.shape)\n",
    "print(df_gen.head(8).to_string(index=False))\n",
    "print('\\nPromotion fraction per SKU:')\n",
    "print(df_gen.groupby('sku_id')['promotion_flag'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and aggregate to daily totals\n",
    "df = pd.read_csv('ecommerce_demand.csv', parse_dates=['date'])\n",
    "agg = df.groupby('date').agg(total_units=('units_sold','sum'), avg_price=('price','mean'), promo_any=('promotion_flag','max'))\n",
    "full_idx = pd.date_range(start=agg.index.min(), end=agg.index.max(), freq='D')\n",
    "agg = agg.reindex(full_idx)\n",
    "agg.index.name = 'date'\n",
    "agg['total_units'] = agg['total_units'].fillna(0).astype(int)\n",
    "agg['avg_price'] = agg['avg_price'].fillna(method='ffill').fillna(method='bfill')\n",
    "agg['promo_any'] = agg['promo_any'].fillna(0).astype(int)\n",
    "\n",
    "TEST_DAYS = 90\n",
    "train_end = agg.index[-(TEST_DAYS + 1)]\n",
    "print('Data range:', agg.index.min().date(), 'to', agg.index.max().date())\n",
    "print('Train ends at:', train_end.date(), 'Test days:', TEST_DAYS)\n",
    "agg.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA: select order and forecast\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "def select_arima_order(series, p_max=3, d_vals=(0,1), q_max=3):\n",
    "    best_aic = float('inf')\n",
    "    best_order = (0,0,0)\n",
    "    for d in d_vals:\n",
    "        for p in range(p_max+1):\n",
    "            for q in range(q_max+1):\n",
    "                try:\n",
    "                    res = ARIMA(series, order=(p,d,q)).fit(method='innovations_mle', disp=0)\n",
    "                    if res.aic < best_aic:\n",
    "                        best_aic = res.aic\n",
    "                        best_order = (p,d,q)\n",
    "                except Exception:\n",
    "                    continue\n",
    "    return best_order\n",
    "\n",
    "series = agg['total_units']\n",
    "train_series = series[:train_end]\n",
    "order = select_arima_order(train_series)\n",
    "print('Selected ARIMA order:', order)\n",
    "model = ARIMA(train_series, order=order)\n",
    "res = model.fit()\n",
    "steps = TEST_DAYS\n",
    "forecast = res.get_forecast(steps=steps).predicted_mean\n",
    "forecast_index = pd.date_range(start=train_series.index[-1] + pd.Timedelta(days=1), periods=steps)\n",
    "arima_forecast = pd.Series(forecast.values, index=forecast_index)\n",
    "\n",
    "# Evaluate\n",
    "actual = series.loc[arima_forecast.index]\n",
    "ARIMA_RMSE = math.sqrt(mean_squared_error(actual.values, arima_forecast.values))\n",
    "ARIMA_MAPE = (np.mean(np.abs((actual.values - arima_forecast.values) / (np.abs(actual.values) + 1e-8))) * 100)\n",
    "print(f'ARIMA RMSE: {ARIMA_RMSE:.4f}, ARIMA MAPE: {ARIMA_MAPE:.2f}%')\n",
    "\n",
    "# Plot ARIMA\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(actual.index, actual.values, label='Actual', color='black')\n",
    "plt.plot(arima_forecast.index, arima_forecast.values, label='ARIMA forecast')\n",
    "plt.title('ARIMA: Actual vs Forecast')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407af872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM: prepare sequences, train, and forecast recursively\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "N_IN = 30\n",
    "features = ['total_units','avg_price','promo_any']\n",
    "arr = agg[features].values.astype(float)\n",
    "train_mask = agg.index <= train_end\n",
    "train_arr = arr[train_mask]\n",
    "test_arr = arr[~train_mask]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_arr)\n",
    "arr_scaled = scaler.transform(arr)\n",
    "\n",
    "# create sliding windows\n",
    "X, y = [], []\n",
    "for i in range(N_IN, len(arr_scaled)):\n",
    "    X.append(arr_scaled[i - N_IN:i, :])\n",
    "    y.append(arr_scaled[i, 0])\n",
    "X = np.array(X); y = np.array(y)\n",
    "train_len = train_arr.shape[0]\n",
    "train_count = max(0, train_len - N_IN)\n",
    "X_train = X[:train_count]\n",
    "y_train = y[:train_count]\n",
    "\n",
    "print('LSTM training samples:', X_train.shape)\n",
    "\n",
    "# build model\n",
    "model = Sequential([LSTM(64, input_shape=(N_IN, len(features))), Dropout(0.2), Dense(32, activation='relu'), Dense(1)])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "EPOCHS = 30\n",
    "es = EarlyStopping(patience=8, restore_best_weights=True, monitor='loss')\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, batch_size=32, callbacks=[es], verbose=1)\n",
    "\n",
    "# recursive forecast on scaled space\n",
    "steps = test_arr.shape[0]\n",
    "last_window = arr_scaled[train_len - N_IN:train_len].copy()\n",
    "preds_scaled = []\n",
    "for s in range(steps):\n",
    "    x = last_window.reshape((1, N_IN, len(features)))\n",
    "    yhat_scaled = model.predict(x, verbose=0)[0,0]\n",
    "    preds_scaled.append(yhat_scaled)\n",
    "    exog_scaled = scaler.transform(test_arr[s].reshape(1,-1))[0]\n",
    "    exog_scaled[0] = yhat_scaled\n",
    "    last_window = np.vstack([last_window[1:], exog_scaled])\n",
    "\n",
    "# inverse transform preds\n",
    "preds_scaled_arr = np.array(preds_scaled).reshape(-1,1)\n",
    "dummy = np.zeros((len(preds_scaled), arr.shape[1]))\n",
    "dummy[:,0:1] = preds_scaled_arr\n",
    "dummy[:,1:] = scaler.transform(test_arr)[:,1:]\n",
    "inv = scaler.inverse_transform(dummy)\n",
    "lstm_preds = inv[:,0]\n",
    "forecast_index = agg.index[agg.index > train_end]\n",
    "lstm_forecast = pd.Series(lstm_preds, index=forecast_index)\n",
    "\n",
    "# Evaluate\n",
    "LSTM_RMSE = math.sqrt(mean_squared_error(agg['total_units'].loc[forecast_index].values, lstm_forecast.values))\n",
    "LSTM_MAPE = (np.mean(np.abs((agg['total_units'].loc[forecast_index].values - lstm_forecast.values) / (np.abs(agg['total_units'].loc[forecast_index].values) + 1e-8))) * 100)\n",
    "print(f'LSTM RMSE: {LSTM_RMSE:.4f}, LSTM MAPE: {LSTM_MAPE:.2f}%')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(agg['total_units'].loc[forecast_index].index, agg['total_units'].loc[forecast_index].values, label='Actual', color='black')\n",
    "plt.plot(lstm_forecast.index, lstm_forecast.values, label='LSTM forecast')\n",
    "plt.title('LSTM: Actual vs Forecast')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a643731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine plots and save, and show metrics\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(agg['total_units'].loc[forecast_index].index, agg['total_units'].loc[forecast_index].values, label='Actual', color='black')\n",
    "plt.plot(arima_forecast.index, arima_forecast.values, label='ARIMA Forecast')\n",
    "plt.plot(lstm_forecast.index, lstm_forecast.values, label='LSTM Forecast')\n",
    "plt.title('Actual vs Forecasted Daily Demand')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Units')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('actual_vs_predicted.png')\n",
    "plt.show()\n",
    "\n",
    "metrics = pd.DataFrame({\n",
    "    'model': ['ARIMA','LSTM'],\n",
    "    'rmse': [ARIMA_RMSE, LSTM_RMSE],\n",
    "    'mape': [ARIMA_MAPE, LSTM_MAPE]\n",
    "})\n",
    "print(metrics)\n",
    "metrics.to_csv('forecast_metrics.csv', index=False)\n",
    "print('Saved actual_vs_predicted.png and forecast_metrics.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa60139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional section: programmatically create another notebook and provide download link (per outline)\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell\n",
    "\n",
    "# Simple example notebook that demonstrates how to serialize and download\n",
    "cells = [\n",
    "    new_markdown_cell('# Generated Notebook\\nThis file was programmatically generated from the main notebook.'),\n",
    "    new_code_cell(\"print('Hello from generated notebook')\")\n",
    "]\n",
    "nb = new_notebook(cells=cells, metadata={\"language_info\": {\"name\": \"python\"}})\n",
    "nbfpath = 'generated_forecast.ipynb'\n",
    "nbformat.write(nb, nbfpath)\n",
    "print('Wrote', nbfpath)\n",
    "\n",
    "# Validate by reading it back\n",
    "nb2 = nbformat.read(nbfpath, as_version=4)\n",
    "print('Generated notebook has cells:', len(nb2['cells']))\n",
    "\n",
    "# Provide download link in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(nbfpath)\n",
    "except Exception:\n",
    "    print('google.colab.files not available in this environment; download manually if needed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8e3ac",
   "metadata": {},
   "source": [
    "## Final notes\n",
    "\n",
    "- Default EPOCHS in LSTM set to 30 for interactive Colab speed; increase to 100 for better performance.\n",
    "- Use GPU runtime in Colab for faster training (Runtime > Change runtime type > GPU).\n",
    "- The notebook is deterministic given the seed, but exact floating-point reproducibility may vary with different TF builds.\n",
    "\n",
    "Thank you â€” adjust parameters at the top of the notebook (SEED, EPOCHS, N_IN) to experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd08029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time features used by improved LSTM\n",
    "agg['dow'] = agg.index.weekday.astype(int)\n",
    "agg['dow_sin'] = np.sin(2 * np.pi * agg['dow'] / 7)\n",
    "agg['dow_cos'] = np.cos(2 * np.pi * agg['dow'] / 7)\n",
    "\n",
    "day_of_year = agg.index.dayofyear.values\n",
    "agg['doy_sin'] = np.sin(2 * np.pi * day_of_year / 365.25)\n",
    "agg['doy_cos'] = np.cos(2 * np.pi * day_of_year / 365.25)\n",
    "\n",
    "agg.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e010469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved LSTM: multi-step (7-day block) forecasting with validation and callbacks\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "N_IN = 30\n",
    "N_OUT = 7\n",
    "features = ['total_units','avg_price','promo_any','dow_sin','dow_cos','doy_sin','doy_cos']\n",
    "arr = agg[features].values.astype(float)\n",
    "\n",
    "train_mask = agg.index <= train_end\n",
    "train_arr = arr[train_mask]\n",
    "test_arr = arr[~train_mask]\n",
    "\n",
    "scaler = MinMaxScaler(); scaler.fit(train_arr)\n",
    "arr_scaled = scaler.transform(arr)\n",
    "\n",
    "# create multi-step windows\n",
    "X, Y = [], []\n",
    "for i in range(N_IN, len(arr_scaled) - N_OUT + 1):\n",
    "    X.append(arr_scaled[i - N_IN:i, :])\n",
    "    Y.append(arr_scaled[i:i+N_OUT, 0])\n",
    "X = np.array(X); Y = np.array(Y)\n",
    "\n",
    "train_len = train_arr.shape[0]\n",
    "train_count = max(0, train_len - N_IN - (N_OUT - 1) + 1)\n",
    "X_train = X[:train_count]\n",
    "Y_train = Y[:train_count]\n",
    "\n",
    "# validation split\n",
    "val_split = max(1, int(0.1 * X_train.shape[0]))\n",
    "X_val = X_train[-val_split:]; Y_val = Y_train[-val_split:]\n",
    "X_train = X_train[:-val_split]; Y_train = Y_train[:-val_split]\n",
    "\n",
    "# build model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "model = Sequential([LSTM(128, input_shape=(N_IN, len(features))), Dropout(0.2), Dense(64, activation='relu'), Dense(N_OUT)])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True), ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7), ModelCheckpoint('colab_lstm_best.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=100, batch_size=32, callbacks=callbacks, verbose=1)\n",
    "\n",
    "# Forecast test horizon in blocks\n",
    "steps = test_arr.shape[0]\n",
    "last_window = arr_scaled[train_len - N_IN:train_len].copy()\n",
    "idx = 0\n",
    "preds_scaled = []\n",
    "while idx < steps:\n",
    "    x = last_window.reshape((1, N_IN, arr.shape[1]))\n",
    "    yhat_block = model.predict(x, verbose=0)[0]\n",
    "    block_len = min(N_OUT, steps - idx)\n",
    "    exog_block = test_arr[idx:idx+block_len]\n",
    "    exog_block_scaled = scaler.transform(exog_block)\n",
    "    exog_block_scaled[:block_len, 0] = yhat_block[:block_len]\n",
    "    preds_scaled.extend(yhat_block[:block_len].tolist())\n",
    "    last_window = np.vstack([last_window[block_len:], exog_block_scaled])\n",
    "    idx += block_len\n",
    "\n",
    "preds_scaled = np.array(preds_scaled).reshape(-1,1)\n",
    "dummy = np.zeros((len(preds_scaled), arr.shape[1]))\n",
    "dummy[:,0:1] = preds_scaled\n",
    "dummy[:,1:] = scaler.transform(test_arr)[:,1:]\n",
    "inv = scaler.inverse_transform(dummy)\n",
    "lstm_preds = inv[:,0]\n",
    "\n",
    "forecast_index = agg.index[agg.index > train_end]\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse_lstm = math.sqrt(mean_squared_error(agg['total_units'].loc[forecast_index].values, lstm_preds))\n",
    "mape_lstm = np.mean(np.abs((agg['total_units'].loc[forecast_index].values - lstm_preds) / (np.abs(agg['total_units'].loc[forecast_index].values) + 1e-8))) * 100\n",
    "print('LSTM RMSE:', rmse_lstm, 'LSTM MAPE:', mape_lstm)\n",
    "\n",
    "# quick plot\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(agg['total_units'].loc[forecast_index].index, agg['total_units'].loc[forecast_index].values, label='Actual', color='black')\n",
    "plt.plot(forecast_index, lstm_preds, label='Improved LSTM (block)', alpha=0.9)\n",
    "plt.legend(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
